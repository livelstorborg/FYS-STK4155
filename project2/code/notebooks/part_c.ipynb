{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb1bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.neural_network import NeuralNetwork\n",
    "from src.activations import Sigmoid, Linear\n",
    "from src.losses import MSE\n",
    "from src.optimizers import GD, RMSprop, Adam\n",
    "from src.training import train\n",
    "from src.metrics import mse\n",
    "from src.utils import runge, polynomial_features, scale_data, OLS_parameters, inverse_scale_y\n",
    "\n",
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febf478",
   "metadata": {},
   "source": [
    "# Setup for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eba4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N = 300\n",
    "x = np.linspace(-1, 1, N)\n",
    "y_true = runge(x)\n",
    "y_noise = y_true + np.random.normal(0, 0.01, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ee4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_nn, y_test_nn = train_test_split(\n",
    "    x.reshape(-1, 1), y_noise.reshape(-1, 1), \n",
    "    test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Scale\n",
    "X_train_s, y_train_s, X_mean, X_std, y_mean = scale_data(X_train_raw, y_train_nn)\n",
    "X_test_s, y_test_s, _, _, _ = scale_data(X_test_raw, y_test_nn, X_mean, X_std, y_mean)\n",
    "\n",
    "y_test_real = inverse_scale_y(y_test_s, y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4466616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SGD] Epoch   0/1000 - Train Loss: 0.0867, Val Loss: 0.0717, Val MSE: 0.0717\n",
      "[SGD] Epoch  10/1000 - Train Loss: 0.0837, Val Loss: 0.0684, Val MSE: 0.0684\n",
      "Early stopping at epoch 14 (patience=10)\n",
      "Neural Network MSE on test set: 0.0682396050584541\n"
     ]
    }
   ],
   "source": [
    "network_input_size = 1\n",
    "layer_output_sizes = [100, 100, 1]  # Two hidden layers with 100 nodes each\n",
    "activations = [Sigmoid(), Sigmoid(), Linear()]\n",
    "loss = MSE()\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "eta = 0.001\n",
    "\n",
    "nn = NeuralNetwork(network_input_size=network_input_size, layer_output_sizes=layer_output_sizes, activations=activations, loss=loss, seed=SEED)\n",
    "train(nn, X_train_s, y_train_s, X_test_s, y_test_s, Adam(eta), \n",
    "      epochs=epochs, batch_size=batch_size, verbose=True, seed=SEED) \n",
    "y_pred_s = nn.predict(X_test_s)\n",
    "y_pred = inverse_scale_y(y_pred_s, y_mean)\n",
    "nn_mse = mse(y_test_real, y_pred)\n",
    "print(f\"Neural Network MSE on test set: {nn_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdb0b3",
   "metadata": {},
   "source": [
    "# Setup for NN with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767a4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.027220\n",
      "Epoch 200/1000, Loss: 0.020118\n",
      "Epoch 300/1000, Loss: 0.006624\n",
      "Epoch 400/1000, Loss: 0.000560\n",
      "Epoch 500/1000, Loss: 0.000174\n",
      "Epoch 600/1000, Loss: 0.000123\n",
      "Epoch 700/1000, Loss: 0.000109\n",
      "Epoch 800/1000, Loss: 0.000133\n",
      "Epoch 900/1000, Loss: 0.000111\n",
      "Epoch 1000/1000, Loss: 0.000125\n",
      "PyTorch MSE: 0.000080\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_s).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_s).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_s).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_s).to(device)\n",
    "\n",
    "# Create DataLoader for mini-batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the neural network\n",
    "class RungeNet(torch_nn.Module):  # ← Use torch_nn\n",
    "    def __init__(self):\n",
    "        super(RungeNet, self).__init__()\n",
    "        self.fc1 = torch_nn.Linear(1, 100)      # ← Use torch_nn\n",
    "        self.fc2 = torch_nn.Linear(100, 100)    # ← Use torch_nn\n",
    "        self.fc3 = torch_nn.Linear(100, 1)      # ← Use torch_nn\n",
    "        self.sigmoid = torch_nn.Sigmoid()       # ← Use torch_nn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = RungeNet().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch_nn.MSELoss()  # ← Use torch_nn\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_pytorch_s = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse scale predictions (same as your code)\n",
    "y_pred_pytorch = inverse_scale_y(y_pred_pytorch_s, y_mean)\n",
    "\n",
    "# Compute MSE (same as your code)\n",
    "pytorch_mse = mse(y_test_real, y_pred_pytorch)\n",
    "\n",
    "print(f\"PyTorch MSE: {pytorch_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e761627",
   "metadata": {},
   "source": [
    "# Comparing our model with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ddce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Our Neural Network MSE: 0.068240\n",
      "PyTorch MSE:             0.000080\n",
      "Difference:              0.068160\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Our Neural Network MSE: {nn_mse:.6f}\")\n",
    "print(f\"PyTorch MSE:             {pytorch_mse:.6f}\")\n",
    "print(f\"Difference:              {abs(pytorch_mse - nn_mse):.6f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741487ee",
   "metadata": {},
   "source": [
    "# Verifying derivatives with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cbdb6",
   "metadata": {},
   "source": [
    "### MSE gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f08570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our gradient:      [ 0.06666667 -0.06666667  0.13333333]\n",
      "Autograd gradient: [ 0.06666667 -0.06666667  0.13333333]\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1.0], [2.0], [3.0]])\n",
    "y_pred = np.array([[1.1], [1.9], [3.2]])\n",
    "\n",
    "loss = MSE()\n",
    "our_loss = loss.forward(y_true, y_pred)\n",
    "our_grad = loss.backward(y_true, y_pred)\n",
    "\n",
    "def mse_auto(y_pred, y_true): \n",
    "    return anp.mean((y_true - y_pred)**2)\n",
    "\n",
    "auto_grad_fn = grad(mse_auto, 0)\n",
    "auto_grad = auto_grad_fn(y_pred, y_true)\n",
    "\n",
    "print(f'Our gradient:      {our_grad.flatten()}')\n",
    "print(f'Autograd gradient: {auto_grad.flatten()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e986a40",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff49323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: 2 -> 3 -> 1\n",
      "Number of layers: 2\n",
      "\n",
      "Layer 0:\n",
      "  Weight gradient max diff: 3.47e-18\n",
      "  Bias gradient max diff:   6.94e-18\n",
      "  ✓ Layer 0 correct: True\n",
      "\n",
      "Layer 1:\n",
      "  Weight gradient max diff: 0.00e+00\n",
      "  Bias gradient max diff:   0.00e+00\n",
      "  ✓ Layer 1 correct: True\n",
      "\n",
      "============================================================\n",
      "✓ ALL GRADIENTS CORRECT: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "test_nn = NeuralNetwork(\n",
    "    network_input_size=2, \n",
    "    layer_output_sizes=[3, 1],\n",
    "    activations=[Sigmoid(), Linear()], \n",
    "    loss=MSE(), \n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "X_test = np.array([[0.5, 0.3], [0.2, 0.8]])\n",
    "y_test = np.array([[1.0], [0.5]])\n",
    "\n",
    "# Compute gradients with backpropagation\n",
    "our_grads = test_nn.compute_gradient(X_test, y_test)\n",
    "\n",
    "# Define forward pass for autograd\n",
    "def nn_loss_auto(layers, X, y, activations):\n",
    "    a = X\n",
    "    for i, ((W, b), act) in enumerate(zip(layers, activations)):\n",
    "        z = anp.dot(a, W.T) + b\n",
    "        # Apply activation\n",
    "        if isinstance(act, Sigmoid):\n",
    "            a = 1 / (1 + anp.exp(-z))\n",
    "        else:  # Linear\n",
    "            a = z\n",
    "    return anp.mean((y - a)**2)\n",
    "\n",
    "# Compute gradients with autograd\n",
    "auto_grad_fn = grad(nn_loss_auto, 0)\n",
    "auto_grads = auto_grad_fn(test_nn.layers, X_test, y_test, test_nn.activations)\n",
    "\n",
    "# Compare gradients for each layer\n",
    "print(f\"Network: {X_test.shape[1]} -> 3 -> 1\")\n",
    "print(f\"Number of layers: {len(our_grads)}\")\n",
    "\n",
    "all_correct = True\n",
    "for layer_idx in range(len(our_grads)):\n",
    "    W_ours, b_ours = our_grads[layer_idx]\n",
    "    W_auto, b_auto = auto_grads[layer_idx]\n",
    "    \n",
    "    w_diff = np.max(np.abs(W_ours - W_auto))\n",
    "    b_diff = np.max(np.abs(b_ours - b_auto))\n",
    "    \n",
    "    layer_correct = (w_diff < 1e-6 and b_diff < 1e-6)\n",
    "    all_correct = all_correct and layer_correct\n",
    "    \n",
    "    print(f\"\\nLayer {layer_idx}:\")\n",
    "    print(f\"  Weight gradient max diff: {w_diff:.2e}\")\n",
    "    print(f\"  Bias gradient max diff:   {b_diff:.2e}\")\n",
    "    print(f\"  ✓ Layer {layer_idx} correct: {layer_correct}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ ALL GRADIENTS CORRECT: {all_correct}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
